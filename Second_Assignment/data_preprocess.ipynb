{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processes: Second Assignment (first part)\n",
    "\n",
    "### Grupo:\n",
    "Víctor Morcuende Castell, 47315589N\n",
    "\n",
    "Guillermo Nájera Lavid, 70845359T\n",
    "\n",
    "Javier Rocamora García, 20081979N\n",
    "\n",
    "Antonio Ruiz García, 06601574E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "from sklearn.impute import SimpleImputer\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "\n",
    "def diagnoseDate_to_ageDiagnosed(birthDate, diagnoseDate):\n",
    "    return pd.to_datetime(diagnoseDate).year-pd.to_datetime(birthDate).year\n",
    "\n",
    "def deathDate_to_survivalTime(diagnosisDate, deathDate):\n",
    "    survivalTime = pd.to_datetime(deathDate).year-pd.to_datetime(diagnosisDate).year\n",
    "    if math.isnan(survivalTime) or survivalTime < 0:\n",
    "        survivalTime = 1000\n",
    "    return survivalTime\n",
    "\n",
    "def deathDate_to_survived(deathDate):\n",
    "    survived = pd.to_datetime(deathDate).year\n",
    "    if math.isnan(survived):\n",
    "        survived = 1\n",
    "    else: \n",
    "        survived = 0\n",
    "    return survived\n",
    "\n",
    "def deathDate_to_age(birthDate, deathDate):\n",
    "    deathAge = pd.to_datetime(deathDate).year-pd.to_datetime(birthDate).year\n",
    "    if math.isnan(deathAge):\n",
    "        deathAge = 0\n",
    "    return deathAge\n",
    "\n",
    "def recurrence_year_to_recurrence_time(diagnosisDate, recurrence_year):\n",
    "    recurrenceTime = recurrence_year-pd.to_datetime(diagnosisDate).year\n",
    "    if math.isnan(recurrenceTime) or recurrenceTime < 0:\n",
    "        recurrenceTime = 1000\n",
    "    return recurrenceTime\n",
    "\n",
    "def fix_pregnancy(pregnancies, abortions, births):\n",
    "    if pregnancies < (births + abortions):\n",
    "        pregnancies = pregnancies+1\n",
    "    return pregnancies\n",
    "\n",
    "def preprocess_t(x):\n",
    "    new_t = x.t\n",
    "    if pd.isnull(new_t):\n",
    "        if not(pd.isnull(x.t_after_neoadj)):\n",
    "            new_t = x.t_after_neoadj\n",
    "        else:\n",
    "            new_t = \"unknown\"\n",
    "    return new_t\n",
    "\n",
    "def preprocess_n(x):\n",
    "    new_n = x.n\n",
    "    if pd.isnull(new_n):\n",
    "        if not(pd.isnull(x.n_after_neoadj)):\n",
    "            new_n = x.n_after_neoadj\n",
    "        else:\n",
    "            new_n = \"unknown\"\n",
    "    return new_n\n",
    "\n",
    "def preprocess_m(x):\n",
    "    new_m = x.m\n",
    "    if pd.isnull(new_m):\n",
    "        if not(pd.isnull(x.m_after_neoadj)):\n",
    "            new_m = x.m_after_neoadj\n",
    "        else:\n",
    "            new_m = \"unknown\"\n",
    "    return new_m\n",
    "\n",
    "def fill_t_after_neoadj(x):\n",
    "    new_t = x.t_after_neoadj\n",
    "    if pd.isnull(new_t):\n",
    "            new_t = x.t\n",
    "    return new_t\n",
    "\n",
    "def fill_n_after_neoadj(x):\n",
    "    new_n = x.n_after_neoadj\n",
    "    if pd.isnull(new_n):\n",
    "            new_n = x.n\n",
    "    return new_n\n",
    "\n",
    "def fill_m_after_neoadj(x):\n",
    "    new_m = x.m_after_neoadj\n",
    "    if pd.isnull(new_m):\n",
    "            new_m = x.m\n",
    "    return new_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of Breast Cancer datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting duplicated data and unused column\n",
    "df1 = pd.read_excel(\"breast_cancer_data.xlsx\")\n",
    "df1 = df1.drop_duplicates(subset=['ehr'], keep='first')\n",
    "df1 = df1.set_index('ehr')\n",
    "df2 = pd.read_excel(\"breast_cancer_data_2.xlsx\")\n",
    "df2 = df2.drop_duplicates(subset=['ehr'], keep='first')\n",
    "df2 = df2.set_index('ehr')\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df.pop('Unnamed: 0')\n",
    "    \n",
    "# Duplicating the DataFrame in order to obtain the numerical variables\n",
    "df_num = pd.DataFrame(data=df, columns=df.columns, index=df.index)\n",
    "df_num.pop('side')\n",
    "df_num.pop('neoadjuvant')\n",
    "df_num.pop('grade')\n",
    "df_num.pop('invasive')\n",
    "df_num.pop('er_positive')\n",
    "df_num.pop('pr_positive')\n",
    "df_num.pop('her2_positive')\n",
    "df_num.pop('hist_type')\n",
    "\n",
    "# Dividing the DataFrame into categorical and numerical variables\n",
    "num_cols = df_num.columns.tolist()\n",
    "df_cat = df.drop(num_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We delete the NULL values of the categorical variables by using the Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat.side = df_cat.side.apply(lambda x: 'unknown' if (x != 'left' and x != 'right') else x)\n",
    "df_cat.invasive = df_cat.invasive.apply(lambda x: 0 if x != 1 else x)\n",
    "\n",
    "# Imputation of nulls in categorical columns using Simple Imputer\n",
    "imp_cat = SimpleImputer(strategy='most_frequent')\n",
    "columns = df_cat.columns\n",
    "index = df_cat.index\n",
    "df_cat = pd.DataFrame(imp_cat.fit_transform(df_cat), columns=columns, index=index)\n",
    "\n",
    "# Transforming categorical values into numerical variables\n",
    "df_cat.neoadjuvant = df_cat.neoadjuvant.apply(lambda x: 0.0 if x == 'no' else 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are about to subsitute the categorical labels that are represented by strings with numerical values, in order to avoid working with Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking some variables out as they are already converted into numerical values\n",
    "df_aux = pd.DataFrame(data=df_cat, columns=df_cat.columns, index=df_cat.index)\n",
    "df_cat.pop('neoadjuvant')\n",
    "df_cat.pop('invasive')\n",
    "df_cat.pop('er_positive')\n",
    "df_cat.pop('pr_positive')\n",
    "df_cat.pop('her2_positive')\n",
    "num_cols = df_cat.columns.tolist()\n",
    "df_aux = df_aux.drop(num_cols, axis=1)\n",
    "\n",
    "# Using OneHotEncoder\n",
    "ohe = preprocessing.OneHotEncoder(sparse=False)\n",
    "df_cat_ohe = pd.DataFrame(ohe.fit_transform(df_cat), \n",
    "                          columns=ohe.get_feature_names_out(df_cat.columns.tolist()),\n",
    "                          index=df_cat.index)\n",
    "\n",
    "# Merge both DataFrames (df_cat_ohe and df_aux)\n",
    "df_cat_def = pd.merge(left=df_cat_ohe, right=df_aux, on='ehr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will swap birth_date, diagnosis_date and death_date with the age at which the patient was diagnosed, their age at the time of death if so, and how long they survived in this case. This will be of way more use to us when trying to use this data for any predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age at which the patient was diagnosed\n",
    "ageDiagnosed = pd.Series(df_num.apply(lambda x: diagnoseDate_to_ageDiagnosed(x.birth_date, x.diagnosis_date), axis=1), name='age_diagnosed')\n",
    "\n",
    "# Time of survival since diagnosis, 1000 in case of full recovery\n",
    "survivalTime = pd.Series(df_num.apply(lambda x: deathDate_to_survivalTime(x.diagnosis_date, x.death_date), axis=1), name='survival_time')\n",
    "\n",
    "# We set \"survived\" column to be the target variable\n",
    "class_col = pd.Series(df_num.apply(lambda x: deathDate_to_survived(x.death_date), axis=1), name='survived')\n",
    "\n",
    "# Recurrence time for a patient\n",
    "recurrenceTime = pd.Series(df_num.apply(lambda x: recurrence_year_to_recurrence_time(x.diagnosis_date, x.recurrence_year), axis=1), name='recurrence_time')\n",
    "\n",
    "# Changing variables\n",
    "df_num.pop('birth_date')\n",
    "df_num.pop('diagnosis_date')\n",
    "df_num.pop('death_date')\n",
    "df_num.pop('recurrence_year')\n",
    "df_num = pd.merge(left=df_num, right=ageDiagnosed, on='ehr')\n",
    "df_num = pd.merge(left=df_num, right=survivalTime, on='ehr')\n",
    "df_num = pd.merge(left=df_num, right=recurrenceTime, on='ehr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will treat differently the pregnancy, abort, birth and caesarean labels, which should not be filled with data computed from the mean of the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.pregnancy = df_num.pregnancy.apply(lambda x: 0 if math.isnan(x) else x)\n",
    "df_num.abort = df_num.abort.apply(lambda x: 0 if math.isnan(x) else x)\n",
    "df_num.birth = df_num.birth.apply(lambda x: 0 if x < 0 else x)\n",
    "df_num.caesarean = df_num.caesarean.apply(lambda x: 0 if math.isnan(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we saw some cases in which number of pregnancies, number of aborts and number of births dont compute, meaning that there may have been less pregnancies than supposed due to contradictory data, we decided to increment the number of pregnancies for those cases that do not add up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.pregnancy = df_num.apply(lambda x: fix_pregnancy(x.pregnancy, x.abort, x.birth), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We delete the NULL values of the numerical variables by using the Simple Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation of nulls in numerical columns using Simple Imputer\n",
    "imp_num = SimpleImputer(strategy='mean')\n",
    "columns = df_num.columns\n",
    "index = df_num.index\n",
    "df_num_def = pd.DataFrame(imp_num.fit_transform(df_num), columns=columns, index=index)\n",
    "\n",
    "# We round up the menarche_age and menopause_age columns to give it sense\n",
    "df_num_def.menarche_age = df_num_def.menarche_age.apply(np.ceil)\n",
    "df_num_def.menopause_age = df_num_def.menopause_age.apply(np.ceil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all variables are numerical and do not have missing values, we can merge the categorical and numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = pd.merge(left=df_cat_def, right=df_num_def, on='ehr')\n",
    "df_preprocessed.to_excel(\"output1.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of TNM datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by eliminating duplicated data, since we do not think that having information about these few cases with multi tumoral cancers will give us any edge in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting duplicated data\n",
    "df3 = pd.read_csv(\"breast_cancer_data_tnm.csv\")\n",
    "df3 = df3.drop_duplicates(subset=['ehr'], keep='last')\n",
    "df3 = df3.set_index('ehr')\n",
    "df4 = pd.read_csv(\"breast_cancer_data_tnm_2.csv\")\n",
    "df4 = df4.drop_duplicates(subset=['ehr'], keep='last')\n",
    "df4 = df4.set_index('ehr')\n",
    "df_tnm = pd.concat([df3, df4], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_after_neoadjuvant, n_after_adjuvant and m_after_neoadjuvant columns will be completed with data from the t, n and m columns for those cases in which there have not been any neoadjuvant treatments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess of t, n and m column, changing IS and X labels and filling with t_after_neoadj in case there is no data\n",
    "df_tnm.t = df_tnm.apply(lambda x: preprocess_t(x), axis=1)\n",
    "df_tnm.n = df_tnm.apply(lambda x: preprocess_n(x), axis=1)\n",
    "df_tnm.m = df_tnm.apply(lambda x: preprocess_m(x), axis=1)\n",
    "\n",
    "# Fill t_after_neoadj, n_after_neoadj and m_after_neoadj column with t column in case there is no data\n",
    "df_tnm.t_after_neoadj = df_tnm.apply(lambda x: fill_t_after_neoadj(x), axis=1)\n",
    "df_tnm.n_after_neoadj = df_tnm.apply(lambda x: fill_n_after_neoadj(x), axis=1)\n",
    "df_tnm.m_after_neoadj = df_tnm.apply(lambda x: fill_m_after_neoadj(x), axis=1)\n",
    "\n",
    "df_tnm.t = df_tnm.t.apply(lambda x: x.replace('.0','') if isinstance(x, str) else int(x))\n",
    "df_tnm.n = df_tnm.n.apply(lambda x: x.replace('.0','') if isinstance(x, str) else int(x))\n",
    "df_tnm.m = df_tnm.m.apply(lambda x: x.replace('.0','') if isinstance(x, str) else int(x))\n",
    "df_tnm.t_after_neoadj = df_tnm.t_after_neoadj.apply(lambda x: x.replace('.0','') if isinstance(x, str) else int(x))\n",
    "df_tnm.n_after_neoadj = df_tnm.n_after_neoadj.apply(lambda x: x.replace('.0','') if isinstance(x, str) else int(x))\n",
    "df_tnm.m_after_neoadj = df_tnm.m_after_neoadj.apply(lambda x: x.replace('.0','') if isinstance(x, str) else int(x))\n",
    "df_tnm = df_tnm.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation of nulls using Simple Imputer\n",
    "imp_tnm = SimpleImputer(strategy='most_frequent')\n",
    "columns = df_tnm.columns\n",
    "index = df_tnm.index\n",
    "df_tnm_def = pd.DataFrame(imp_tnm.fit_transform(df_tnm), columns=columns, index=index)\n",
    "df_tnm_def.to_excel(\"output2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the columns from tnm dataset into categorical ones after preprocessing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OneHotEncoder\n",
    "df_tnm_def_ohe = pd.DataFrame(ohe.fit_transform(df_tnm_def), \n",
    "                          columns=ohe.get_feature_names_out(df_tnm_def.columns.tolist()),\n",
    "                          index=df_tnm_def.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed_pre_def = pd.merge(left=df_preprocessed, right=df_tnm_def_ohe, how='outer', on='ehr')\n",
    "\n",
    "# Imputation of the new nulls (caused by merging 2 different datasets) using Simple Imputer\n",
    "imp_df = SimpleImputer(strategy='most_frequent')\n",
    "columns = df_preprocessed_pre_def.columns\n",
    "index = df_preprocessed_pre_def.index\n",
    "df_preprocessed_def = pd.DataFrame(imp_df.fit_transform(df_preprocessed_pre_def), columns=columns, index=index)\n",
    "df_preprocessed_def.to_excel(\"output_def.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08f2a05ac046e84805368809866a6b709c3cc9ec790929dec02dbab1889d195a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
